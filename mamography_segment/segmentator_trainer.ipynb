{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from tqdm import tqdm\n",
    "\n",
    "import osfrom pathlib import Path\n",
    "\n",
    "import numpy as npfrom PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86636dc3",
   "metadata": {},
   "source": [
    "### paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for patient folder structure\n",
    "data_dir = \"data\"  # Organized dataset folder\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_model_path = os.path.join(save_dir, \"best_segmentation_model.pth\")\n",
    "\n",
    "val_dir = Path(data_dir) / \"val\"\n",
    "\n",
    "# Define paths for train and validation splitstrain_dir = Path(data_dir) / \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5eab90",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientFolderDataset(Dataset):\n",
    "    def __init__(self, split_dir, image_size=(256, 256)):\n",
    "        self.split_dir = Path(split_dir)\n",
    "        self.image_size = image_size\n",
    "        self.patient_folders = []\n",
    "        \n",
    "        # Find all patient folders in the split directory\n",
    "        if self.split_dir.exists():\n",
    "            self.patient_folders = [f for f in self.split_dir.iterdir() if f.is_dir()]\n",
    "        \n",
    "        # Validate each patient folder has exactly 2 images\n",
    "        self.valid_patients = []\n",
    "        for patient_folder in self.patient_folders:\n",
    "            image_files = list(patient_folder.glob(\"*.jpg\")) + list(patient_folder.glob(\"*.jpeg\")) + list(patient_folder.glob(\"*.png\"))\n",
    "            \n",
    "\n",
    "            if len(image_files) == 2:    val_dataset = None\n",
    "\n",
    "                # Sort by file size - larger is usually original, smaller is mask    print(f\"âŒ Validation directory not found: {val_dir.absolute()}\")\n",
    "\n",
    "                img1, img2 = sorted(image_files, key=lambda x: x.stat().st_size, reverse=True)else:\n",
    "\n",
    "                original_img = img1    val_dataset = PatientFolderDataset(val_dir)\n",
    "\n",
    "                mask_img = img2if val_dir.exists():\n",
    "\n",
    "                \n",
    "\n",
    "                self.valid_patients.append((original_img, mask_img))    train_dataset = None\n",
    "\n",
    "            print(f\"âŒ Training directory not found: {train_dir.absolute()}\")\n",
    "\n",
    "        print(f\"Found {len(self.valid_patients)} valid patient cases in {split_dir}\")else:\n",
    "\n",
    "            train_dataset = PatientFolderDataset(train_dir)\n",
    "\n",
    "    def __len__(self):if train_dir.exists():\n",
    "\n",
    "        return len(self.valid_patients)# Create datasets\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):        return {\"image\": image, \"mask\": mask}\n",
    "\n",
    "        original_path, mask_path = self.valid_patients[idx]        \n",
    "\n",
    "                mask = torch.tensor(mask).unsqueeze(0)    # Shape: (1, H, W)\n",
    "\n",
    "        # Load and preprocess original image        image = torch.tensor(image).unsqueeze(0)  # Shape: (1, H, W)\n",
    "\n",
    "        image = Image.open(original_path).convert('L')  # Convert to grayscale        # Convert to tensors and add channel dimension\n",
    "\n",
    "        image = image.resize(self.image_size, Image.Resampling.LANCZOS)        \n",
    "\n",
    "        image = np.array(image, dtype=np.float32) / 255.0  # Normalize to [0, 1]        mask = (mask > 0.5).astype(np.float32)  # Binarize\n",
    "\n",
    "                mask = np.array(mask, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        # Load and preprocess mask        mask = mask.resize(self.image_size, Image.Resampling.NEAREST)  # Use nearest for masks\n",
    "        mask = Image.open(mask_path).convert('L')  # Convert to grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df464a31",
   "metadata": {},
   "source": [
    "### dataloaders init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4458c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "if train_dataset and len(train_dataset) > 0:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    print(f\"âœ… Training loader created with {len(train_dataset)} samples\")\n",
    "else:\n",
    "\n",
    "    print(\"âŒ No training data found! Please run extract_malignant_cases.py first.\")    val_loader = None\n",
    "\n",
    "    train_loader = None    print(\"âŒ No validation data found! Please run extract_malignant_cases.py first.\")\n",
    "\n",
    "else:\n",
    "\n",
    "if val_dataset and len(val_dataset) > 0:    print(f\"âœ… Validation loader created with {len(val_dataset)} samples\")\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7eb0d",
   "metadata": {},
   "source": [
    "### model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e736eb1",
   "metadata": {},
   "source": [
    "### train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have data before starting training\n",
    "if train_loader is None or val_loader is None:\n",
    "    print(\"âŒ Cannot start training without data!\")\n",
    "    print(\"\\nðŸ“‹ Next steps:\")\n",
    "    print(\"1. Run extract_malignant_cases.py to organize malignant patient folders\")\n",
    "    print(\"2. Check that the data directory structure is correct\")\n",
    "    print(\"3. Ensure patient folders contain exactly 2 images each (original + mask)\")\n",
    "else:\n",
    "    print(f\"ðŸš€ Starting training with {len(train_dataset)} training samples and {len(val_dataset)} validation samples\")\n",
    "    \n",
    "    num_epochs = 20\n",
    "    best_dice = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "    \n",
    "    for batch in train_bar:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        masks = batch[\"mask\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # ===== Validation =====\n",
    "    model.eval()\n",
    "    dice_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "        for batch in val_bar:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            masks = batch[\"mask\"].to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            dice_metric(y_pred=preds, y=masks)\n",
    "        \n",
    "        val_dice = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_loss/len(train_loader):.4f} | Val Dice: {val_dice:.4f}\")print(f\"Best Dice score: {best_dice:.4f} | Saved at {best_model_path}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "    # Save best model\n",
    "\n",
    "    if val_dice > best_dice:        print(f\"âœ… Best model saved (Dice: {best_dice:.4f})\")\n",
    "\n",
    "        best_dice = val_dice        torch.save(model.state_dict(), best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
